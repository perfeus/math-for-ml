{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will develop our own classification algorithm based on probabilistic models.\n",
    "\n",
    "In the `data` folder you can find a collection of texts in English, Italian, Spanish, German, French, Polish and Portuguese languages obtained from random Wikipedia articles, see e.g. `Spanish.txt`. (See corresponding `.source.txt` files and links therein for lists of authors.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomas zapata sierra medellin colombia  de mayo de  es productor de cine y de teatrocita requerida\n",
      "sad eyed lady of the lowlands en espanol senorita de ojos tristes de las tierras bajas es una cancion compuesta por el cantante estadounidense bob dylan fue incluida en el album blonde on blonde editado el  de mayo de \n",
      "la revista mojo la coloco en el puesto  de su lista de las  mejores canciones de bob dylan\n",
      "calyptocephalella canqueli es una especie extinta de anfibio anuro perteneciente al genero c\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Spanish.txt\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riedhofe ist der name von ortsteilen in deutschland\n",
      "\n",
      "in badenwurttemberg\n",
      "riedhofe langenau ortsteil der stadt langenau im albdonaukreis\n",
      "riedhofe frickingen ortsteil der gemeinde frickingen im bodenseekreis\n",
      "riedhofe riegel am kaiserstuhl ortsteil der gemeinde riegel am kaiserstuhl im landkreis emmendingen\n",
      "riedhofe kongen ortsteil der gemeinde kongen im landkreis esslingen\n",
      "riedhofe leingarten ortsteil der gemeinde leingarten im landkreis heilbronn\n",
      "riedhofe bad wurzach ortsteil der stadt bad wurzac\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/German.txt\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our training data. These texts are preprocessed: only standard Latin characters kept, diacritics removed, punctuations removed, all letters converted to lowercase. We will use similar preprocessing for new texts that we have to classify. Here are some useful functions to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "### FROM: https://stackoverflow.com/a/518232/3025981\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "### END FROM\n",
    "\n",
    "def clean_text(s):\n",
    "    return re.sub(\"[^a-z \\n]\", \"\", strip_accents(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning: obtaining character frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to find character relative frequencies in texts of each language. We will consider them as probability for character to appear in our multinomial model. Let's write function `get_freqs(text, relative)` that takes string `text` as input and returns dictionary which keys are all distinct characters occurred in `text` and values are frequencies (relative if `relative` is `True` and absolute otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a31da5fc73be6eee36e6a8d49d08885",
     "grade": false,
     "grade_id": "cell-bcf26fb5354210c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_freqs(text, relative=False):\n",
    "    counter = {}\n",
    "    for element in text:\n",
    "        if element not in counter:\n",
    "            counter[element] = 0\n",
    "        counter[element] += 1\n",
    "    if relative:\n",
    "        for element in counter:\n",
    "            counter[element] = counter[element] / len(text)\n",
    "    return counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea90921753fb298894ed69e10cdaea3a",
     "grade": true,
     "grade_id": "cell-122fc498050c5088",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert get_freqs('Hello, World!') == {'H': 1, 'e': 1, 'l': 3, 'o': 2, ',': 1,\n",
    "                                      ' ': 1, 'W': 1, 'r': 1, 'd': 1, '!': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use function `get_freqs` to create a dictionary `lang_to_prob` which keys are names of languages (i.e. `'English'`, `'Italian'`, `'Spanish'`, `'German'`, `'French'`, `'Polish'`, `'Portuguese'`) and values are dictionaries of relative frequencies, obtained by processing of corresponding `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7cf421e2337fd84ea7d773c2badd767",
     "grade": false,
     "grade_id": "cell-1beba6e8ac03b0b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.07760728328040059, 'l': 0.03626398545723105, 'e': 0.09363580376183861, 'x': 0.001387857977519179, 'i': 0.06683527488881646, ' ': 0.16778707284643476, 'r': 0.055831543781342974, 'y': 0.013557637617890481, 'o': 0.059573803685010765, 'm': 0.02255145297577816, 'n': 0.06324295321307709, '\\n': 0.004925656661284587, 'v': 0.008451063755965, 'c': 0.028972774439621363, 'h': 0.036203266670714586, 'u': 0.021785652770325615, 's': 0.05419957149884944, 'k': 0.006603477823392594, 'b': 0.01411649828562365, 'f': 0.018519229887521547, 'd': 0.030964102805579683, 't': 0.06663700946345659, 'g': 0.01564190290198625, 'w': 0.012623311800882032, 'p': 0.01732591985863675, 'j': 0.0023073138876256354, 'z': 0.0016604729373890178, 'q': 0.0007881050658055339}\n"
     ]
    }
   ],
   "source": [
    "lang_to_probs = {}\n",
    "languages = [\"English\", \"Italian\", \"Spanish\", \"German\", \"French\", \"Polish\", \"Portuguese\"]\n",
    "for lang in languages:\n",
    "    with open(\"data/\" + lang + \".txt\") as f:\n",
    "        lang_to_probs[lang] = get_freqs(f.read(), relative=True)\n",
    "\n",
    "print(lang_to_probs[\"English\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77eb7631d066c05a174abd522974d57e",
     "grade": true,
     "grade_id": "cell-56feb4b2ab62c15c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(lang_to_probs['Polish']['a'] - 0.08504245058355897) < 0.00001\n",
    "assert abs(lang_to_probs['English']['x'] - 0.001387857977519179) < 1e-5\n",
    "assert len(set(lang_to_probs['Portuguese'])) == 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihoods\n",
    "Now let us start implementing the actual classifier. We begin with a multinomial likelihood function. Implement function `multinomial_likelihood(probs, freqs)` that takes two arguments: `probs` are dictionary of probabilities of each character (in some language) and `freqs` is dictionary of absolute frequencies of each character (in some text we want to classify). This function has to return probability to obtain these absolute frequencies from multinomial distribution with given probabilities $P((X_1 = f_1) \\cap (X_2 = f_2) \\cap \\ldots \\cap (X_k = f_k))$ provided that $(X_1, \\ldots, X_k)$ is a system of multinomially distributed values with probabilities $(p_1, \\ldots, p_k)$. (We need the `factorial` function that can be imported from `math`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d1cc2f77a90c6103bc87d746c969174",
     "grade": false,
     "grade_id": "cell-9a0431bd007d04cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import factorial as fact\n",
    "\n",
    "def multinomial_likelihood(probs, freqs):\n",
    "    \"\"\"\n",
    "    the function calculate the probability to get particular frequency of characters in some language P(Lan|freq)\n",
    "    \"\"\"\n",
    "    numerator = fact(sum(freqs.values()))\n",
    "    denominator = 1\n",
    "    for item in freqs.values():\n",
    "        denominator *= fact(item)\n",
    "    res = map(lambda x, y: x ** y, probs.values(), freqs.values())\n",
    "    prob = 1\n",
    "    for i in res:\n",
    "        prob *= i\n",
    "        \n",
    "    return numerator/denominator * prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find likelihood of data with frequencies `{'a': 2, 'b': 1, 'c': 2}` and probabilities `{'a': 0.2, 'b': 0.5, 'c': 0.3}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05400000000000001"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3}, freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "878cba6cdee3f20b13be7533627e0553",
     "grade": true,
     "grade_id": "cell-0a8a6d99346b6bf8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.054) < 0.000001\n",
    "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.1, 'c': 0.3, 'd': 0.4},\n",
    "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.0108) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coefficient with factorials depends only on `freqs` (i.e. text that we analyse) and does not depend on `probs`. It means that for all possible languages this coefficient will be the same. As we are going to consider fixed text and compare likelihoods for different languages, we see that we do not need this coefficient in most cases. Let us implement the function `multinomial_likelihood_without_coeff` that returns the same probability as `multinomial_likelihood` but without the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88623d0902e7a18eac1bd7885c210973",
     "grade": false,
     "grade_id": "cell-99bfc1e01b6b804e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_likelihood_without_coeff(probs, freqs):\n",
    "    res = map(lambda x, y: x ** y, probs.values(), freqs.values())\n",
    "    prob = 1\n",
    "    for i in res:\n",
    "        prob *= i\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding probability becomes smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018000000000000004"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                                     freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a71414f54c182fd492b07fb428fc71c6",
     "grade": true,
     "grade_id": "cell-3de0433027c5af68",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert multinomial_likelihood_without_coeff(\n",
    "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
    "    freqs={'a': 2, 'b': 1, 'c': 2}) == 0.00324\n",
    "assert abs(multinomial_likelihood_without_coeff(\n",
    "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
    "    freqs={'a': 2, 'b': 1, 'c': 5}) - 8.747999999999e-05) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, likelihoods become extremely small very quickly when we increase absolute frequencies in data. It is not surprising: the probability to get the text that coincides with our actual text from a random experiment we discussed is extremely small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00018000000000000004"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 3, 'b': 2, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.866455078125001e-10"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 3, 'b': 20, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limited precision of computer arithmetic, for frequencies large enough we will get exactly zero likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                                     freqs={'a': 543, 'b': 512, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we usually cannot use likelihoods like this directly. Common way to deal with such a tiny numbers is to use _logarithms_ instead of likelihood themself. Indeed, logarithm is a monotonically increasing function. If we compare logarithms, it is equivalent to compare their arguments.\n",
    "\n",
    "### Log likelihood\n",
    "Implement function `log_likelihood_without_coeff(probs, freqs)` that calculates logarithm of likelihood (without factorial coefficient). Note that you cannot simply put `multinomial_likelihood_without_coeff` into `log`: if the likelihood would be extremely small (equal to zero from the computer's point of view), `log` of it will be negative infinity. Thus we have to algebraically transform the expression for log likelihood first. Let us use properties of logarithm to do it: $\\log (ab) = \\log a + \\log b$, $\\log (a^b)=b\\log a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5519c92583bc940cad0f293a20a80397",
     "grade": false,
     "grade_id": "cell-c6283b47bbcf5b95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def log_likelihood_without_coeff(probs, freqs):\n",
    "    res = sum(map(lambda x, y: y * log(x), probs.values(), freqs.values()))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihoods are probabilities, so they are less than 1 and their logarithms are negative. The larger absolute value of log-likelihood, the less is likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.319968614080018"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deal with the inputs that lead to exact zero value previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1231.2240885070605"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6de307b4a1af6955524f2381c00ec68",
     "grade": true,
     "grade_id": "cell-38d4478fc7c1656b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                              freqs={'a': 2, 'b': 1, 'c': 2}) + 6.319968614080018) < 0.00001\n",
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512, 'c': 2}) + 1231.2240885070605) < 0.0001\n",
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512}) + 1228.8161428984085) < 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical part: cross-entropy and Jensen's inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we obtained is also known as **cross entropy** and is extremely popular in machine learning, namely, in classification problems. It allows you to measure how good probability distribution $(p_1, \\ldots, p_k)$ fits to actual absolute frequencies obtained from the data $(f_1, \\ldots, f_k)$.\n",
    "\n",
    "Assume that frequencies $(f_1, \\ldots, f_k)$ are fixed. What is the best distribution $(p_1, \\ldots, p_k)$ from likelihood's perspective?\n",
    "\n",
    "Intuitively, it seems that we have to put relative frequencies\n",
    "$$r_i = \\frac{f_i}{\\sum_{j=1}^k f_j}$$ \n",
    "as $p_i$ to get best fit. In fact, it is true. To prove that we can use Jensen's inequality for logarithms. It is stated as follows:\n",
    "\n",
    "For any values $\\alpha_1, \\ldots, \\alpha_k$, such that $\\sum_{j=1}^k \\alpha_j = 1$ and $\\alpha_j \\ge 0$ for all $j=1, \\ldots, k$, and any positive values $x_1, \\ldots, x_k$, the following inequality holds:\n",
    "\n",
    "$$\\log \\sum_{j=1}^k \\alpha_j x_j \\ge \\sum_{j=1}^k \\alpha_j\\log(x_j).$$\n",
    "\n",
    "Using this inequality we can say that\n",
    "$$\\sum_{j=1}^k r_j \\log p_j - \\sum_{j=1}^k r_j \\log r_j \\le 0,$$\n",
    "then to obtain maximum log-likelihood (and therefore maximum likelihood) for fixed $(f_1, \\ldots, f_k)$ we have to put $p_i=r_i$, $i=1,\\ldots, k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use likelihood to choose the best language for some text we want to classify. Let's write function `mle_best(text, lang_to_probs)` that takes some `text` and dictionary `lang_to_probs` that we created previously and returns the name of the language such that the likelihood of our data for this language is maximal. Note that you have to preprocess  `text` using the function `clean_text` before finding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "489b2d5186054873ecb269d587c09e37",
     "grade": false,
     "grade_id": "cell-761e62cc0f17dc2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mle_best(text, lang_to_probs):\n",
    "    \"\"\"\n",
    "    the function assigns probability from language to each char of the given text\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    freqs_text = get_freqs(text)\n",
    "    result = {}\n",
    "    for language, relativeFreqDict in lang_to_probs.items():\n",
    "        probs_text = {char:relativeFreqDict[char] for char in freqs_text.keys()}\n",
    "        result[language] = log_likelihood_without_coeff(probs_text, freqs_text)\n",
    "    \n",
    "    return max(zip(result.values(), result.keys()))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://en.wikipedia.org/wiki/1134_Kepler\n",
    "text = \"\"\"1134 Kepler, provisional designation 1929 SA, is a stony asteroid \n",
    "and eccentric Mars-crosser from the asteroid belt, approximately \n",
    "4 kilometers in diameter\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polish'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://pl.wikipedia.org/wiki/(1134)_Kepler\n",
    "text = \"\"\"\"(1134) Kepler – planetoida z grupy przecinających \n",
    "orbitę Marsa okrążająca Słońce w ciągu 4 lat i 145 dni \n",
    "w średniej odległości 2,68 au.\n",
    "\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://it.wikipedia.org/wiki/1134_Kepler\n",
    "text = \"\"\"1134 Kepler è un asteroide areosecante. Scoperto nel 1929, \n",
    "presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \n",
    "UA e da un'eccentricità di 0,4651458, inclinata di 15,17381° rispetto\n",
    "\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Portugues\n",
    "text = \"Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Portugues\n",
    "text = \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#French\n",
    "text = \"UA e da un'eccentricità di 0,4651458, inclinata\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polish'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Italian\n",
    "text = \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polish\n",
    "text = \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English\n",
    "text = \"and eccentric Mars-crosser from the\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df4d8728e5b8d79bbe87bcf0244ea19b",
     "grade": true,
     "grade_id": "cell-b83cf8d4bcee2ab1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "assert mle_best(lines[0], lang_to_probs) == 'Portuguese'\n",
    "assert mle_best(lines[1], lang_to_probs) == 'Portuguese'\n",
    "assert mle_best(lines[2], lang_to_probs) == 'French'\n",
    "assert mle_best(lines[3], lang_to_probs) == 'Italian'\n",
    "assert mle_best(lines[4], lang_to_probs) == 'Polish'\n",
    "assert mle_best(lines[5], lang_to_probs) == 'English'\n",
    "assert mle_best(\"Aaaa\", lang_to_probs) == \"Portuguese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look, it works!\n",
    "Fantastic! Just a couple of equations, and we created fully automatic language detection algorithm!\n",
    "\n",
    "Now let us make it even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go Bayes\n",
    "Assume that we selected random article from all Wikipedia articles in the languages that we consider. There are different numbers of articles in different languages, so it is more likely to obtain an article e.g. in English than in Polish (at least, at this moment). It means that we need stronger evidence for Polish to accept it compared with English. To take into account this information, we will use Bayes' rule as discussed in the videos.\n",
    "\n",
    "Recall that in the Bayesian approach we consider _prior_ probabilities of languages, then use Bayes' rule to find their posterior probabilities and select the language with the largest posterior probability. We begin with finding prior probabilities. Let's create a dictionary `lang_to_prior` whose keys are language names and values are prior probabilities. Assume that priors are proportional to the number of articles in each language (in thousands). Let us use these numbers (in thousands): English: 6090, Italian: 1611, Spanish: 1602, German: 2439, French: 2222, Polish: 1412, Portuguese: 1034. Remember that all prior probabilities should sum up to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "702d53d0bce478d6f65fbc96b0edb9a6",
     "grade": false,
     "grade_id": "cell-1076e90562dad99c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 0.3711151736745887, 'Italian': 0.09817184643510055, 'Spanish': 0.09762340036563072, 'German': 0.1486288848263254, 'French': 0.13540524070688603, 'Polish': 0.08604509445460086, 'Portuguese': 0.06301035953686776}\n"
     ]
    }
   ],
   "source": [
    "lang_to_prior = {}\n",
    "languagesDict = {\"English\": 6090, \"Italian\": 1611, \"Spanish\": 1602, \"German\": 2439, \"French\": 2222, \"Polish\": 1412, \"Portuguese\": 1034}\n",
    "for key in languagesDict:\n",
    "    lang_to_prior[key] = languagesDict[key] / sum(languagesDict.values())\n",
    "print(lang_to_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11e4f59d59762b514409a549f44504f6",
     "grade": true,
     "grade_id": "cell-19765ecb964a7f7f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(lang_to_prior['French'] - 0.13540524070688603) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to implement function `bayesian_best(text, lang_to_probs, lang_to_prior)` that takes some text `text`, dictionary `lang_to_probs` created before and `lang_to_prior` with prior probabilities. This function has to return the language name with the largest posterior probability. Note that as we only compare posterior probabilities, we can ignore the denominator in Bayes' rule: it is the same for all languages. Note also that we need to work with logarithms of posterior instead of posteriors themself to avoid extremely small numbers. Use properties of logarithm to do it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216727f5af4f793b64690cff1cda4c51",
     "grade": false,
     "grade_id": "cell-1725db02a8cb77ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_best(text, lang_to_probs, lang_to_prior):\n",
    "    # prepare the text for the following processing\n",
    "    text = clean_text(text)\n",
    "    # create dictionary with keys as  chars in the text and values as the number of chars appeared in the text\n",
    "    freqs_text = get_freqs(text)\n",
    "    \n",
    "    # create variables as dictionaries for the intermediant result of processing\n",
    "    conditionalProb = {}\n",
    "    languageProb = {}\n",
    "    # for the 1st part we create dictionary with keys as chars from the text and values as probability in each language\n",
    "    # related to char in the text appeared to calculate conditional probability\n",
    "    # for the 2nd part we create dictionary with conditional probability calculated\"P(A|H)\"\n",
    "    for language, relativeFreqDict in lang_to_probs.items():\n",
    "        # 1st part\n",
    "        probs_text = {char:relativeFreqDict[char] for char in freqs_text.keys()}\n",
    "        # 2nd part\n",
    "        conditionalProb[language] = log_likelihood_without_coeff(probs_text, freqs_text)\n",
    "    \n",
    "    # in this step we are finding a probability of each language separately\n",
    "    languageProb = {key: conditionalProb[key] + log(lang_to_prior[key]) for key in lang_to_prior.keys()}\n",
    "    \n",
    "    return max(zip(languageProb.values(), languageProb.keys()))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 UA e da un'eccentricità di 0,4651458, inclinata Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i\"\n",
    "bayesian_best(text, lang_to_probs, lang_to_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, MLE algorithm believes that word `\"The\"` belongs go German language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_best(\"The\", lang_to_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we take into account that English is more popular in Wikipedia, the results changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_best(\"The\", lang_to_probs, lang_to_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3df29258fc53bca55abeea29456124f",
     "grade": true,
     "grade_id": "cell-d6b6b54026f24fb0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "answers = ['English', 'English', 'French', 'Italian', 'Polish', 'English']\n",
    "for line, answer in zip(lines, answers):\n",
    "    assert bayesian_best(line, lang_to_probs, lang_to_prior) == answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring uncertainty\n",
    "Finally, let us get posterior probability of how certain our Bayesian algorithm is in its classification. Implement function `bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang)` that takes some `text`, `lang_to_probs`, `lang_to_prior` and a particular language `test_lang` we are interested in and returns posterior probability for this language provided this text.\n",
    "\n",
    "Note that in this case we have to work with actual probabilities (likelihoods), not their logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf52836c632b7285e461a95c4e15cac1",
     "grade": false,
     "grade_id": "cell-8705f891f8eff4f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang):\n",
    "    # prepare the text for the following processing\n",
    "    text = clean_text(text)\n",
    "    # create dictionary with keys as  chars in the text and values as the number of chars appeared in the text\n",
    "    freqs_text = get_freqs(text)\n",
    "    # create variables as dictionaries for the intermediant result of processing\n",
    "    conditionalProb = {}\n",
    "    totalProb = {}\n",
    "    \n",
    "    # for the 1st part we create dictionary with keys as chars from the text and values as probability in each language\n",
    "    # related to char in the text appeared to calculate conditional probability\n",
    "    # for the 2nd part we create dictionary with conditional probability calculated\"P(A|H)\"\n",
    "    for language, relativeFreqDict in lang_to_probs.items():\n",
    "        # 1st part\n",
    "        probs_text = {char:relativeFreqDict[char] for char in freqs_text.keys()}\n",
    "        # 2nd part\n",
    "        conditionalProb[language] = multinomial_likelihood_without_coeff(probs_text, freqs_text)\n",
    "\n",
    "    # in this step we are finding a probability of each language separately\n",
    "    totalProb = sum({key: conditionalProb[key] * lang_to_prior[key] for key in lang_to_prior.keys()}.values())\n",
    "    \n",
    "    return (conditionalProb[test_lang] * lang_to_prior[test_lang]) / totalProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2686381464045339"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.534626604689872"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36596793151737517"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12095519926831602"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cec1793d2b6f5143a7bbaf9e1c3f0bd",
     "grade": true,
     "grade_id": "cell-23ecc58928d8d618",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\") - 0.26863814640) < 0.00001\n",
    "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\") - 0.534626604) < 0.00001\n",
    "assert abs(bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\") - 0.365967931517) < 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our algorithm believes that `\"Das\"` belongs to English. This is due to the small amount of data (only three letters!) and high prior for English. However, it is not very certain: the posterior is only 0.37. On the other hand, `\"The\"` belongs to `\"English\"` with much larger posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Today we used our knowledge of probability to construct our own classifier algorithm. Actually, it is a version of a well-known naive Bayesian classifier. Of course, this classifier is far from perfect: for example, it completely ignores words and deals only with characters and their frequencies. Nevertheless, it works rather well despite being so simple. Also it shows several important concepts: likelihood, maximum likelihood estimation, Bayesian estimations and so on.\n",
    "\n",
    "Now you are ready for more sophisticated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P.S. Efficiency considerations\n",
    "During this project we used dictionaries to represent frequency tables and pure Python constructions like loops to process them. However, it is much more efficient to use `numpy` arrays and vectorized functions. You can try to redo this project with `numpy` if you know how to use it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
